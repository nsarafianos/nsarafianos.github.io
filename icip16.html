<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Nikolaos Sarafianos</title>
    <meta name="viewport" content="width=device-width">
    <link rel="canonical" href="https://nsarafianos.github.io/">
      
    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/main.css">
  </head>

  <body>

<div class="page-content">
<div class="wrap">
<div class="post">
<header class="post-header">
    <h1>ICIP 2016: Deep Learning Trends and Open Problems</h1>
    <p class="meta">Nikolaos Sarafianos, October 11, 2016</p>
</header>

Last week I spent four days in Phoenix AZ, attending ICIP 2016 and presenting <a href="https://www.researchgate.net/publication/307516192_Show_me_your_body_Gender_classification_from_still_images">my paper</a>. Below, I summarize some interesting papers and a few open problems that caught my attention. Disclaimer: since I work on Machine and Deep Learning I focused on these types of papers than more traditionally image-related. </br></br>

    
<font size="4">Contents
<ul>
    <li><a href="#dltutorial">Deep Learning Tutorial by Google</a></li>
    <li><a href="#opensettutorial">The Open Set Recognition Problem Tutorial by Anderson Rocha</a></li>
    <li><a href="#dlpapers">Deep Learning Papers</a></li>
    <li><a href="#conc">Conclusions and Open Questions</a></li>
</ul>
</font>     
    

<a name="dltutorial"></a>
<h3>Deep Learning Tutorial by Google</h3>
    The conference started with a packed tutorial on Deep Learning from <a href="http://research.google.com/pubs/JonathonShlens.html">Jon Shlens</a> and <a href="http://research.google.com/pubs/author38233.html">George Toderici</a> from Google. They covered in great breadth (surprisingly they didn't go deep) the field of Deep Learning from AlexNet to batch normalization, to ResNets to the Inception v4.  The presentation covered image embeddings, RNNs and LSTMs, autoencoders, action recognition from videos (for example the 2-stream paper of Simonyan and Zisserman) and many others that you can find in the <a href="https://goo.gl/Ahdexv"> slides</a>. [The link is from George Toderici's Google Drive; let me know if it stops working]</br></br>
    
Some recent papers from their slides that I didn't know about and I found interesting were: 
<ul>
    <li><a href="https://arxiv.org/pdf/1603.04530v1.pdf">Object Contour Detection with a Fully Convolutional Encoder-Decoder Network</a> in which the authors proposed an end-to end learning method to perform object contour detection using an encoder-decoder network. Interestingly, they used parts of VGG-16 for the encoder part and set up  the decoder part in reverse by using bigger filter sizes (5x5 instead of 3x3) but the same feature maps in a descending order. 
    <figure>
    <img src="assets/ConvDeconv.png" width="800">
        </figure></br>
    </li>
    <li><a href="https://arxiv.org/pdf/1503.08909v2.pdf">Beyond Short Snippets: Deep Networks for Video Classification</a> in which the authors proposed two video-classification methods capable of aggregating the output of a ConvNet at each frame into video-level predictions. They experimented with many feature pooling techniques along with an architecture of five stacked LSTM layers. 
    <figure>
    <img src="assets/FeaturePooling.png" width="400">
    </figure></br>
    </li>
    <li><a href="https://arxiv.org/pdf/1504.06852v2.pdf">FlowNet: Learning Optical Flow with Convolutional Networks</a> in which the authors propose a neat architecture to find the optical flow using ConvNets. Their method includes a refinement step in which they get high resolution predictions from coarse feature maps at different levels of the ConvNet. 
    <figure>
    <img src="assets/FlowNet.png" width="700">
    </figure></br>
    </li>
    <li><a href="https://arxiv.org/pdf/1604.03650v1.pdf">Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks</a> in which a very interesting method is proposed to generate 3D images that can be viewed with a VR headset. 
    <figure>
    <img src="assets/Deep3D.png" width="700">
    </figure></br>
    </li>

    <li><a href="http://ivpl.eecs.northwestern.edu/sites/default/files/07444187.pdf">Video Super-Resolution With Convlutional Neural Networks</a> in which the authors proposed a video super-resolution method and investigated different options of combining the video frames within one CNN architecture. 
    <figure>
    <img src="assets/superResolution.png" width="800">
    </figure></br>  
    </li>
    <li><a href="https://arxiv.org/pdf/1601.06759v3.pdf.pdf">Pixel Recurrent Neural Networks</a> which blew my mind because they employed RNNs to predict pixels in both spatial dimensions. Residual connections were also employed to alleviate for the 12-layer deep network. 
    <figure>
    <img src="assets/PixelCNN.png" width="500">
    </figure></br> 
    </li>
    <li><a href="https://arxiv.org/pdf/1608.05148v1.pdf">Full Resolution Image Compression with Recurrent Neural Networks</a> in which the authors used RNNs for image (yes image, not video) compression. In each iteration the obtain the residual between the input and the reconstruction and the provide that residual as an input to the next pass to get a higher quality reconstruction and a smaller residual. Besides the paper itself you can read a <a href="https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html">nice blog</a> written by the authors that explains how their method works.</li>
</ul></br>
    
Before moving forward there are some exciting news, datasets, and tutorials: 
<ul>
    <li>In case you missed it, the ImageNet 2016 results <a href="http://image-net.org/challenges/LSVRC/2016/results">were released</a> last week with the top method reporting classification accuracy of less than 3%.</li>
    <li>Jon Shlens has a <a href="https://arxiv.org/pdf/1404.1100v1.pdf">tutorial on PCA</a>. I hadn't come across it so far and it's very informative.</li>
    <li><a href="https://research.googleblog.com/2016/09/introducing-open-images-dataset.html">Open Images Dataset</a> which comprises ~9 million URLs to images that have been annotated with labels spanning over 6000 categories. </li>
    <li><a href="https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html">YouTube-8M Dataset</a> which comprises 8 million YouTube video URLs  along with video-level labels from a diverse set of 4800 Knowledge Graph entities. </li>
</ul>        
<a name="opensettutorial"></a>
<h3>The Open Set Recognition Problem Tutorial by Anderson Rocha</h3>    
Another very interesting tutorial talk was offered later the same day by Anderson Rocha on Open Set Recognition and the difficulties that arise in such a scenario. You can find the slides <a href="https://recodbr.files.wordpress.com/2016/09/2016-icip-tutorial-open-set-light.pdf">here</a>. In closed set recognition we train our algorithms with data that correspond to a specific set of classes (for example breeds of dogs) and at test time we present to our model unseen data which however belong to one of the known classes. In open set recognition, our knowledge of the world at training time is incomplete and data that belong to unknown classes can be fed to an algorithm at test time. 

<figure>
    <img src="assets/openSet.png" width="600">
</figure></br> 

In the presentation they formalized "openness", open space risk and employed a method using the statistical extreme value theory. Some interesting papers and ideas that attracted my attention and utilize these techniques were: 
<ul>
<li><a href="http://www.wjscheirer.com/papers/wjs_tpami2011_metarecognition.pdf">Meta-Recognition, the Theory and Practice of Recognition Score Analysis</a></li>
<li><a href="http://www.wjscheirer.com/papers/wjs_tpami2013_openset.pdf">Towards Open Set Recognition</a></li>
<li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Bendale_Towards_Open_World_2015_CVPR_paper.pdf">Towards Open World Recognition</a></li>
<li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bendale_Towards_Open_Set_CVPR_2016_paper.pdf">Towards Open Set Deep Networks </a></li>
</ul>

<a name="dlpapers"></a>
<h3>Deep Learning Papers</h3>
<ul>
<li><a href="http://www.eecs.qmul.ac.uk/~xz303/papers/ICIP16/WangZhuXiangGong_ICIP2016.pdf">Towards Unsupervised Open-Set Person Re-Identification</a></li>
<li><a href="https://hal.archives-ouvertes.fr/hal-01372216/document">Max-Min convolutional neural networks for image classification</a>
which is similar to <a href="https://arxiv.org/pdf/1603.05201v2.pdf">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units</a></li>
<li><a href="http://www.cvc.uab.es/LAMP/joost/wp-content/papercite-data/pdf/cervantes2016partproposals.pdf">Hierarchical Part Detection with Deep Neural Networks </a></li>
<li><a href="https://hal.archives-ouvertes.fr/hal-01374118/document">Speeding-up a convolutional neural network by connecting an SVM network</a></li>
<li><a href="https://infoscience.epfl.ch/record/218496/files/ICIP_CAMERAREADY_2715.pdf">Adaptive data augmentation for image classification</a></li>
<li><a href="https://arxiv.org/pdf/1605.06695v1.pdf">Fine-to-coarse Knowledge Transfer For Low-Res Image Classification</a></li>

</ul>
<a name="conc"></a>
<h3>Conclusion</h3>
In a post cpu and maybe gpu world there's a chance that models will be trained in FPGAs as Rico Malvar from MSR <a href="http://arstechnica.com/information-technology/2016/09/programmable-chips-turning-azure-into-a-supercomputing-powerhouse/">showed us</a> or in <a href="https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html">Tensor Processing Units</a>. On another note, I saw a lot of papers trying to propose new methods and architectures by training ConvNets from scratch with few thousands of training samples. 
Andrej Karpathy addressed this during the Bay Area Deep Learning school by posing the following Q&A: </br></br>
Q: How do I know which architecture to use?</br>
A: Don't be a hero. Take whatever works on ILSVRC (such as the ResNet), download the pre-trained model, add/delete some parts and fine-tune to your application. </br></br>

Finally, some take home messages from the tutorials and panels I attended are related to learning paradigms. Besides supervised learning that most of the papers are trying to address, there's tons of  interest on how to learn with a few labels per class (semi-supervised) or in an unsupervised (Yann LeCun has a nice slide with a cake describing this). To make this happen the fields of transfer learning (transfer knowledge from one domain to another by recycling models and distilling information) and active learning (a testing sample of an unseen class shows up: why not keep it and start learning it?) are going to be huge in the near future.  

</div>
</div>
</div>
<footer class="site-footer">
</footer>
</body>
</html>
