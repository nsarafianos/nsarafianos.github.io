<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Nikolaos Sarafianos</title>
    <meta name="viewport" content="width=device-width">
    <link rel="canonical" href="https://nsarafianos.github.io/">
      
    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/main.css">
  </head>

  <body>

<div class="page-content">
<div class="wrap">
<div class="post">
<header class="post-header">
    <h1>Fairness Accountability and Transparency Conference </h1>
    <p class="meta">Nikolaos Sarafianos, February, 26, 2018</p>
</header>
        
I was fortunate to attend the <a href="https://fatconference.org/2018/">FAT*</a> in NYC, a 2-day multi-disciplinary conference in which ~450 people got together from fields such social sciences, law and machine learning to discuss and present works on ethics, fairness, justice, interprertability and transparency and how these intersect with the machine learning models that we build. It was such a refreshing experience since I got to talk with people who work in AI policy, with lawers and social scientists and attend inspiring and thought provoking keynotes and talks. Below I will try to summarize all the notes I kept and the cool things I learned in just two days. You can find the all the procedings <a href="http://proceedings.mlr.press/v81/">here</a>. 

</br></br>

<b>TL;DR</b>: FAT* is wonderful, you will get to talk with tons of people outside your field, exchange ideas, think of problems that you're working on from a different perspective. Since it's going to take place next year as well I would encourage you to apply or even better submit a paper. 
    
</br></br>
    <b>Keynotes</b>:
    The conference started with <a href="https://dataprivacylab.org/people/sweeney/">Latanya Sweeney's</a> keynote in which she made some very compelling arguments in the intersection of privacy and technology. She mentioned that the video cameras of the past did not have a mute button (a technological decision) that enabled people to record things without permission and also be accused of illegal wiretapping (a law-related decision). One of her main points is that we live in a technocratic era in which the design of new technology leads to laws (and not the other way around) and that technocracy leads to unforseen consequences with sometimes adverse impact. She gave plenty of examples of ad-targeting algorithms that are biased against specific groups. For example in ads about arrest black people have 80% more chances to have the "arrested" word in the ad, or SAT pricing can be higher in aeras with a high percentage of asian population. How research impacts policy, how and when should regulation respond to tech were open questions throughout her talk. When asked about what would be the first thing she would like to see in our algorithms she responded with the transparency and full examination of bias in all parts of the process (data and modeling). The talks will be posted online but in the meantime there are some similar talks of hers on YouTube (for example the <a href="https://www.youtube.com/watch?v=UBzP0NouiGo">Grace Hopper one</a>) that you can check out. 
</br> </br>
The second keynote was delivered by <a href="https://content.law.virginia.edu/faculty/profile/dh9ev/2299809">Deborah Hellman</a> and discussed what discrimination is, when is it wrong and why. Aiming to distinguish justice and fairness she started her talk with this story: "During campus protests of the 1960s, Sidney Morgenbesser was hit on the head by police. When asked whether he had been treated unfairly or unjustly, he responded that it was "unfair, but not unjust. It was unfair because they hit me over the head, but not unjust because they hit everyone else over the head." She brought examples in which discrimination based on attributes such as the age in the driving test is considered fine by everyone although we do not treat everybody equally. Age after all is not the perfect proxy for driving she argued and by making such a choice we perform a wrongful discrimination. Her first hypothesis was "Treat like cases alike". She argued that people are like and unlike to one another. For example she mentioned a <a href="https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html">NYT article</a> that pointed out that "relative to men with similar criminal histories, women are significantly less likely to commit future violent acts" and thus, if we treated similar cases alike we would end up with harsher decisions for women. She also brought up an excellent point about compouding injustice an example of which could be higher insurance rates for victims of child abuse which led to an even better question from the audience if this could also include "compouding misfortune". Is treating like cases alike an empty argument she asked. Should it be supplemented by the purpose of the law and assign the punishment that each one deserves? Referencing Peter Westen's Empty Idea of Equality she argued that inaccuracy is a non-comparative notion (given A was charged with X, B can be charged with Y) which is not unfair. Her talk was full of interesting points and raised several questions that have difficult answers. 
</br></br>
<b>Talks</b>: A few talks addressed privacy, and discminiation in online ads. The paper of Speicher et al. on targeted advertising discrimination discussed how sensitive attributes such as race, age or gender can be used by targeted advertising and that even when such options are not available you can easily find others that are highly correlated with specific groups. For example anonymized state data can be cross-checked with voting databases in some states which can lead to identifying some people of interest, or that by look-alike ad targeting can scale the bias significanlty. The work of Datta et al. included a brilliant discussion between a technologist and a lawyer about Google's ad targeting (for a career opportunity customized mainly for men). Let's say that company X puts an ad on Google aiming to target only males and then Google targets mainly males. Is this legal or not they asked? What if the same company wanted the same ad to be directed to all sexes equally but Google's algorithms knew that it should be targeted to men because for example most high-earners/CEOs are male. They brought up an argument for responsibility vs capability which translates to lack of control versus lack of incentives. <a href="https://www.eugdpr.org/">GDPR</a> was a central topic of discussion and it will continue to be in the next few years after it takes effect this spring. 

</br></br>
    <b>Acknowledgments</b>: I'd like to thank Solon Barocas and the rest of the organizers for awarding me a travel grant to attend this great conference. 
</br></br>
<div align="right">
  <a href="https://nsarafianos.github.io/">Back to my page</a>
</div>





</div>
</div>
</div>
</body>
</html>
